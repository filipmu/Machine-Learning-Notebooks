{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# GPT-2 XL Text Generation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "print(torch.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import GPT2Config\n",
    "from transformers import GPT2LMHeadModel, GPT2Tokenizer\n",
    "\n",
    "from tqdm import trange\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "def top_k_top_p_filtering(logits, top_k=0, top_p=0.0, filter_value=-float('Inf')):\n",
    "    \"\"\" Filter a distribution of logits using top-k and/or nucleus (top-p) filtering\n",
    "        Args:\n",
    "            logits: logits distribution shape (batch size x vocabulary size)\n",
    "            top_k > 0: keep only top k tokens with highest probability (top-k filtering).\n",
    "            top_p > 0.0: keep the top tokens with cumulative probability >= top_p (nucleus filtering).\n",
    "                Nucleus filtering is described in Holtzman et al. (http://arxiv.org/abs/1904.09751)\n",
    "        From: https://gist.github.com/thomwolf/1a5a29f6962089e871b94cbd09daf317\n",
    "    \"\"\"\n",
    "    top_k = min(top_k, logits.size(-1))  # Safety check\n",
    "    if top_k > 0:\n",
    "        # Remove all tokens with a probability less than the last token of the top-k\n",
    "        indices_to_remove = logits < torch.topk(logits, top_k)[0][..., -1, None]\n",
    "        logits[indices_to_remove] = filter_value\n",
    "\n",
    "    if top_p > 0.0:\n",
    "        sorted_logits, sorted_indices = torch.sort(logits, descending=True)\n",
    "        cumulative_probs = torch.cumsum(F.softmax(sorted_logits, dim=-1), dim=-1)\n",
    "\n",
    "        # Remove tokens with cumulative probability above the threshold\n",
    "        sorted_indices_to_remove = cumulative_probs > top_p\n",
    "        # Shift the indices to the right to keep also the first token above the threshold\n",
    "        sorted_indices_to_remove[..., 1:] = sorted_indices_to_remove[..., :-1].clone()\n",
    "        sorted_indices_to_remove[..., 0] = 0\n",
    "\n",
    "        # scatter sorted tensors to original indexing\n",
    "        indices_to_remove = sorted_indices_to_remove.scatter(dim=1, index=sorted_indices, src=sorted_indices_to_remove)\n",
    "        logits[indices_to_remove] = filter_value\n",
    "    return logits\n",
    "\n",
    "\n",
    "def sample_sequence(model, length, context, num_samples=1, temperature=1, top_k=0, top_p=0.0,\n",
    "                    repetition_penalty=1.0, device='cpu'):\n",
    "    context = torch.tensor(context, dtype=torch.long, device=device)\n",
    "    context = context.unsqueeze(0).repeat(num_samples, 1)\n",
    "    generated = context\n",
    "    with torch.no_grad():\n",
    "        #for _ in trange(length):\n",
    "        for _ in range(length):\n",
    "\n",
    "            inputs = {'input_ids': generated}\n",
    "\n",
    "            outputs = model(**inputs)  # Note: we could also use 'past' with GPT-2/Transfo-XL/XLNet/CTRL (cached hidden-states)\n",
    "            next_token_logits = outputs[0][:, -1, :] / (temperature if temperature > 0 else 1.)\n",
    "\n",
    "            # repetition penalty from CTRL (https://arxiv.org/abs/1909.05858)\n",
    "            for i in range(num_samples):\n",
    "                for _ in set(generated[i].tolist()):\n",
    "                    next_token_logits[i, _] /= repetition_penalty\n",
    "                \n",
    "            filtered_logits = top_k_top_p_filtering(next_token_logits, top_k=top_k, top_p=top_p)\n",
    "            if temperature == 0: # greedy sampling:\n",
    "                next_token = torch.argmax(filtered_logits, dim=-1).unsqueeze(-1)\n",
    "            else:\n",
    "                next_token = torch.multinomial(F.softmax(filtered_logits, dim=-1), num_samples=1)\n",
    "            generated = torch.cat((generated, next_token), dim=1)\n",
    "    return generated"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#tokenizer = GPT2Tokenizer.from_pretrained(\"gpt2\")\n",
    "#model = GPT2LMHeadModel.from_pretrained(\"gpt2\")\n",
    "tokenizer = GPT2Tokenizer.from_pretrained(\"gpt2-xl\")\n",
    "model = GPT2LMHeadModel.from_pretrained(\"gpt2-xl\")\n",
    "#tokenizer = GPT2Tokenizer.from_pretrained(\"/media/SSD/OPENGPT-2/nips-papers/output\")\n",
    "#model = GPT2LMHeadModel.from_pretrained(\"/media/SSD/OPENGPT-2/nips-papers/output\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(42)\n",
    "torch.manual_seed(42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "  model.to(torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\"));"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.eval();"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "while True:\n",
    "    print(\"\\n\\n\")\n",
    "    raw_text = input(\"Model prompt >>> \")\n",
    "    context_tokens = tokenizer.encode(raw_text, add_special_tokens=False)\n",
    "    out = sample_sequence(\n",
    "        model=model,\n",
    "        context=context_tokens,\n",
    "        num_samples=1,\n",
    "        length=200,\n",
    "        temperature=1.0,\n",
    "        top_k=0,\n",
    "        top_p=0.9,\n",
    "        repetition_penalty=1.0,\n",
    "        device=torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\"),\n",
    "        )\n",
    "    out = out[:, len(context_tokens):].tolist()\n",
    "    for o in out:\n",
    "        text = tokenizer.decode(o, clean_up_tokenization_spaces=True)\n",
    "        #text = text[: text.find(args.stop_token) if args.stop_token else None]\n",
    "        text = text[: None]\n",
    "\n",
    "    print(\"\\n\\n AI output:\\n\",text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Examples\n",
    "\n",
    "\n",
    "**Model prompt >>> My dad is an expert at growing orchids.  He has a special room with high humidity to grow them.  Some of the most\n",
    "\n",
    "\n",
    " **AI output:**\n",
    "  impressive orchids I've seen are grown in his room in Texas.  His goal is to grow hundreds of plants from these plants with a few left over in his storage.  He explains that before, it wasn't as easy to grow these plant, but as the decades have gone by, new technologies have come to help grow these plants.  He gets a dozen plants from one single orchid and has to buy so much fertilizer.\n",
    "\n",
    "Unlike many orchids, lilacs have upright (not hanging) leaves.  For this reason they don't need as much moisture and do a better job of growing faster.   He plants from cuttings and has taken off quite a few years since I was a kid.   He first began planting from cuttings when I was 3.  It was also when I was a teenager that I started experimenting with flowers.  I did only have a punnet tube and used cuttings I had left over from the previous year\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "**Model prompt >>> The scientists studied the dancing chikens, goats, and pigs and developed a theory that they published in the journal Nature.\n",
    "\n",
    "\n",
    " **AI output:**\n",
    "  That theory states that a brain area called the cerebral cortex directs the brain's dance movements.\n",
    "\n",
    "\"What we believe is that the brains of rodents and birds and probably a host of other creatures are utilizing their own dance patterns as an 'automatic pilot,'\" said lead author Dr. John Cacioppo.\n",
    "\n",
    "In this case, that means that the muscles are doing the driving. The brain receives a signal telling it that it is time to dance. The motor cortex then takes over, activating muscle fibers to propel the animal, and slowly stepping its way through the gaps between movements.\n",
    "\n",
    "The study says that it is only when animals have worked out what movements they are using that they are able to link movements with speech.\n",
    "\n",
    "\"If you wanted to dance, it would be extremely hard to copy your movements,\" said Dr. Paul Turkalo, a neuroscientist who studies the structure and function of the human brain at the State University of New York in Buffalo. \"You'd have\n",
    "\n",
    "**Model prompt >>> What time is it?\n",
    "\n",
    "\n",
    " **AI output:**\n",
    "  That confusion @-@ high will make all the difference in an early season episode ; the boss will have to wake up and figure out how to get them all right. \" He continued, \" It was a show we wanted to do in Season 6, so we were thinking what if we didn 't [ break out ]? How could we let them form and do all this weird little fly through the air. \"\n",
    " \n",
    " = = Relationship = = \n",
    " \n",
    " In the season premiere, that would become the romance between Jean and Raven with Scully. While moving into the hospital when she was wounded, Jean finds her love's most treasured memory, Scully tells her not to mind. However, while at the same time questioning Raven, Jean ( who is also the Beatrix of'hit series ) is attracted to a patient. In one scene, she convinces Dr. Baderik to give her a flat to sleep on at night. Raven later revealed to him that she\n",
    "\n",
    "\n",
    "\n",
    "**Model prompt >>> In September 2010 , a teaser website was\n",
    "\n",
    "\n",
    " **AI output:**\n",
    "  developed that called itself the The Next Key of the Moon, with the release of the comic at the beginning of November 2012. The campaign continues with a live stream of the upcoming events @-@ featuring the entire Moon with a portion of it aired on On Xtra. As of April 2014, The Next Key of the Moon remains in development, and features fifty million extra viewers. \n",
    " \n",
    " = = Merchandise = = \n",
    " \n",
    " Later in 2011, a teaser webisode ran for the premiere of the series. This included a special feature where they shared on with merchandise who had joined the cult. \n",
    " In February 2012, an exclusive promotional video released for Star Trek : The Next Generation was released, featuring the words \" We love the show, you know, don 't you just want to be there \" and the musical \" You Got Me Back in the Night \". As part of that release, NBC released new series episodes of the series featuring Dr. McCoy, and on\n",
    "\n",
    "\n",
    "\n",
    "**Model prompt >>> I remeber the day my Dad made kool aid with flour instead of sugar.\n",
    "\n",
    "\n",
    " **AI output:**\n",
    "  I was only seven years old. But I remember every part of that day.\n",
    "\n",
    "So there's always this strange thing going on. We've been trying to tell you about it all of this time.\n",
    "\n",
    "We didn't want you to miss the big time. Now I have the chance to tell you how that big time came to be. A year ago, all of you had hoped and prayed that life would finally catch up with our baby. The baby that you expected to see today.\n",
    "\n",
    "But todayâ€¦ we're telling you. Because you have waited for this, too. But only for a couple weeks. The only way we can hold you back is for this to be here with you for the rest of your life.\n",
    "\n",
    "Imagine us showing you to the stand at Popeye's. On a white sand beach, ten feet high. That's where we took you all those years ago. Now I'm holding you again. Inside the heart of a magnificent chicken.\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
